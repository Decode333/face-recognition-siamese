{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMi2FjxapJfq2K5GHXU8nrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decode333/face-recognition-siamese/blob/main/face_recognition_siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qry5JgUC_uq-",
        "outputId": "a8e04bb0-5621-4ec5-ba95-49a55ec6cdc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import random"
      ],
      "metadata": {
        "id": "5otqoniOA-RI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder_path, image_size=(100, 100)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for person_name in os.listdir(folder_path):\n",
        "        person_folder = os.path.join(folder_path, person_name)\n",
        "        if not os.path.isdir(person_folder):\n",
        "            continue\n",
        "        for img_name in os.listdir(person_folder):\n",
        "            img_path = os.path.join(person_folder, img_name)\n",
        "            img = load_img(img_path, target_size=image_size)\n",
        "            img = img_to_array(img) / 255.0\n",
        "            images.append(img)\n",
        "            labels.append(person_name)\n",
        "    return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "X6ZngMlFBXxT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pairs(images, labels):\n",
        "    pair_images = []\n",
        "    pair_labels = []\n",
        "\n",
        "    # Get unique class labels\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    label_to_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
        "\n",
        "    for idx, img in enumerate(images):\n",
        "        current_label = labels[idx]\n",
        "        pos_idx = idx\n",
        "        while pos_idx == idx:\n",
        "            pos_idx = random.choice(label_to_indices[current_label])\n",
        "\n",
        "        neg_label = random.choice(unique_labels[unique_labels != current_label])\n",
        "        neg_idx = random.choice(label_to_indices[neg_label])\n",
        "\n",
        "        # Positive pair\n",
        "        pair_images.append([img, images[pos_idx]])\n",
        "        pair_labels.append(1)\n",
        "\n",
        "        # Negative pair\n",
        "        pair_images.append([img, images[neg_idx]])\n",
        "        pair_labels.append(0)\n",
        "\n",
        "    return np.array(pair_images), np.array(pair_labels)"
      ],
      "metadata": {
        "id": "iKivURddBW1x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load and prepare dataset ---\n",
        "data_folder = \"dataset\"  # Change if your dataset folder is different\n",
        "images, labels = load_images_from_folder(data_folder)\n",
        "pair_images, pair_labels = create_pairs(images, labels)"
      ],
      "metadata": {
        "id": "7eYcJChUA-Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets\n",
        "split_idx = int(0.8 * len(pair_images))\n",
        "trainX, testX = pair_images[:split_idx], pair_images[split_idx:]\n",
        "trainY, testY = pair_labels[:split_idx], pair_labels[split_idx:]"
      ],
      "metadata": {
        "id": "JU9Gme2-A-Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_siamese_network(input_shape=(100, 100, 3)):\n",
        "    embedding_model = build_embedding_model(input_shape)\n",
        "\n",
        "    input_a = layers.Input(shape=input_shape)\n",
        "    input_b = layers.Input(shape=input_shape)\n",
        "\n",
        "    emb_a = embedding_model(input_a)\n",
        "    emb_b = embedding_model(input_b)\n",
        "\n",
        "    distance = layers.Lambda(lambda tensors: tf.math.abs(tensors[0] - tensors[1]))([emb_a, emb_b])\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
        "\n",
        "    siamese_model = Model([input_a, input_b], outputs)\n",
        "\n",
        "    return siamese_model\n",
        "\n",
        "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    \"\"\"\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    square_pred = tf.math.square(y_pred)\n",
        "    margin_square = tf.math.square(tf.math.maximum(margin - y_pred, 0))\n",
        "    return tf.math.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
      ],
      "metadata": {
        "id": "4kMEw-5HB077"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build Siamese model ---\n",
        "input_shape = images.shape[1:]\n",
        "model = build_siamese_network(input_shape)\n",
        "model.compile(loss=contrastive_loss, optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "l9tLeWGQA-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train the model ---\n",
        "history = model.fit(\n",
        "    [trainX[:, 0], trainX[:, 1]],\n",
        "    trainY,\n",
        "    validation_data=([testX[:, 0], testX[:, 1]], testY),\n",
        "    batch_size=32,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "jO7pXnkIA-Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save the entire model ---\n",
        "model.save(\"siamese_model.h5\")"
      ],
      "metadata": {
        "id": "AvtOyIyLA-Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save the embeddings for known faces ---\n",
        "embedding_model = model.get_layer('Embedding')\n",
        "known_embeddings = []\n",
        "known_names = []\n",
        "\n",
        "# Create known database\n",
        "for label in np.unique(labels):\n",
        "    person_indices = np.where(labels == label)[0]\n",
        "    person_img = images[person_indices[0]]  # Take one image per person\n",
        "    person_img = np.expand_dims(person_img, axis=0)\n",
        "    emb = embedding_model.predict(person_img)[0]\n",
        "    known_embeddings.append(emb)\n",
        "    known_names.append(label)\n",
        "\n",
        "# Save to pickle\n",
        "with open(\"embeddings.pickle\", \"wb\") as f:\n",
        "    pickle.dump({\"embeddings\": known_embeddings, \"names\": known_names}, f)"
      ],
      "metadata": {
        "id": "TnMsKRrNA-BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pahF1A9SA9u7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}